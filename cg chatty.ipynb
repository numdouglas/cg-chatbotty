{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0902 15:06:03.890734  9388 deprecation_wrapper.py:119] From c:\\program files\\python37\\lib\\site-packages\\tflearn\\helpers\\summarizer.py:9: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
      "\n",
      "W0902 15:06:03.895274  9388 deprecation_wrapper.py:119] From c:\\program files\\python37\\lib\\site-packages\\tflearn\\helpers\\trainer.py:25: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "W0902 15:06:04.040055  9388 deprecation_wrapper.py:119] From c:\\program files\\python37\\lib\\site-packages\\tflearn\\collections.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0902 15:06:04.135046  9388 deprecation_wrapper.py:119] From c:\\program files\\python37\\lib\\site-packages\\tflearn\\config.py:123: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "W0902 15:06:04.320826  9388 deprecation_wrapper.py:119] From c:\\program files\\python37\\lib\\site-packages\\tflearn\\config.py:129: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
      "\n",
      "W0902 15:06:04.322834  9388 deprecation_wrapper.py:119] From c:\\program files\\python37\\lib\\site-packages\\tflearn\\config.py:131: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# things we need for NLP\n",
    "# NLTK is a leading platform for building Python programs to work with human language data.\n",
    "# It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet,\n",
    "# along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing,\n",
    "# and semantic reasoning.\n",
    "import nltk\n",
    "# A computer program or subroutine that stems word may be called a stemming program, stemming algorithm, or stemmer.\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "# things we need for Tensorflow\n",
    "# Numpy is a library for the Python programming language, adding \n",
    "# support for large, multi-dimensional arrays and matrices, along with a \n",
    "# large collection of high-level mathematical functions thttp://localhost:8888/notebooks/Documents/Notes/cg%20chatty%202/cg%20chatbotty/cg%20chatty.ipynb#o operate on these arrays.\n",
    "import numpy as np\n",
    "# TFlearn is a modular and transparent deep learning library built on top of Tensorflow.\n",
    "# It was designed to provide a higher-level API to TensorFlow in order to facilitate and speed-up experimentations,\n",
    "# while remaining fully transparent and compatible with it.\n",
    "import tflearn\n",
    "# Tensorflow is an open source machine learning library, which is used to develop and train ML models\n",
    "import tensorflow as tf\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import our chat-bot intents file\n",
    "import json\n",
    "intent_file= open('intents.json','r')\n",
    "intents = json.load(intent_file)\n",
    "intent_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131 documents\n",
      "28 classes ['hours', 'finfinancials', 'opentoday', 'goodbye', 'greeting', 'get_services', 'soft_issues_response_errors', 'departments', 'get_info', 'management_structure', 'soft_issues', 'transaction', 'Ebanking', 'bug_resolve_affirmed', 'payments_n_clearing', 'fins_trans', 'EntSolutions', 'e_trans', 'clearing_trans', 'ent_trans', 'services', 'soft_issues_response_noantivirus', 'bug_per', 'location_question', 'payments', 'thanks', 'soft_issues_response_soft_antivirus', 'soft_issues_response_hard']\n",
      "116 unique stemmed words ['when', 'nop', 'ceo', 'on', 'ebank', 'bye', 'finfin', 'much', 'enlight', 'depart', 'day', 'op', 'hi', 'good', 'wher', 'nee', 'end', 'pay', 'hierachy', 'am', 'up', 'tak', 'mor', 'tour', 'serv', ',', 'ther', 'hav', 'perform', 'do', 'about', 'program', 'yo', 'man', 'read', 'who', 'no', 'fintech', 'with', 'hard', 'today', 'hello', 'the', '2', 'field', 'off', 'an', 'deal', 'of', 'ok', 'let', \"'s\", 'inform', 'yeah', 'cash', 'card', 'in', 'clear', 'ye', 'i', 'sort', 'you', 'wil', 'what', 'is', 'phys', 'has', 'acceiv', 'see', 'may', 'problem', 'yep', 'find', 'goodby', 'enterpr', 'settl', 'visit', 'want', 'lat', 'keny', 'softw', 'posit', 'can', 'out', 'near', 'for', 'er', 'solv', 'opt', 'neg', 'provid', 'hour', 'expery', 'know', 'how', 'loc', 'help', 'tel', 'mastercard', 'structure', 'credit', 'a', 'exceiv', 'explain', 'to', 'me', 'thank', 'anyon', 'it', 'clos', 'ar', 'consult', 'that', 'issu', 'bug', 'org']\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "classes = []\n",
    "references = []\n",
    "words_to_ignore = ['?','!']\n",
    "# loop through each sentence in our intents patterns\n",
    "for intent in intents['intents']:\n",
    "    #note arr['name'] is visiting that array by name \n",
    "    for pattern in intent['question_patterns']:\n",
    "        # add to list containing the classes of words\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])\n",
    "        # tokenize each word in the sentence\n",
    "#Tokenization is the act of breaking up a sequence of strings into pieces such as words, keywords, phrases, symbols and other elements called tokens\n",
    "        word = nltk.word_tokenize(pattern)\n",
    "        # append word to our words list\n",
    "        words.extend(word)\n",
    "        # add to documents in our corpus \n",
    "#         A corpus is a collection of machine-readable texts that have been produced in a natural communicative setting\n",
    "#in the references multidim list of bot, append a list of words at index 0 and a list of contents of intents with key 'tag'\n",
    "        references.append((word, intent['tag']))\n",
    "        \n",
    "\n",
    "# stem and lower each word and remove duplicates\n",
    "words = [ stemmer.stem(word.lower()) for word in words if word not in words_to_ignore]\n",
    "#set method removes the duplicates in the words and classes obtained\n",
    "words = list(set(words))\n",
    "classes = list(set(classes))\n",
    "\n",
    "print (len(references), \"documents\")\n",
    "print (len(classes), \"classes\", classes)\n",
    "print (len(words), \"unique stemmed words\", words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our training data array bags\n",
    "training = []\n",
    "output = []\n",
    "# create an empty array for our output\n",
    "null_output = [0] * len(classes)\n",
    "#A vocabulary is a set of all words known to our bot\n",
    "# training set, vocabulary of words for each sentence\n",
    "#In the following method, we will convert the list of words we have to an array of tensors that tensorflow can use\n",
    "for ref in references:\n",
    "    # initialize our vocabulary of words\n",
    "    vocab = []\n",
    "    # list of tokenized words for the pattern\n",
    "    pattern_words = ref[0]\n",
    "    # stem each word, to its root(s) :-)\n",
    "    pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]\n",
    "    # create our vocabulary of words array\n",
    "    for w in words:\n",
    "        vocab.append(1) if w in pattern_words else vocab.append(0)\n",
    "\n",
    "    # output is a '0' for inactive and '1' for active tag\n",
    "    output_row = list(null_output)\n",
    "    output_row[classes.index(ref[1])] = 1\n",
    "#append to multidim array training the vocabulary of root forms and the row they each correspond to\n",
    "    training.append([vocab, output_row])\n",
    "\n",
    "# shuffle our features and turn into np.array\n",
    "#shuffling will reduce the variance and overfitting\n",
    "random.shuffle(training)\n",
    "training = np.array(training)\n",
    "\n",
    "# create train and test lists\n",
    "#the : stands for everything from the beginning to end , 0 represents the dimension, the 1st in 1d array\n",
    "train_x = list(training[:,0])\n",
    "train_y = list(training[:,1])\n",
    "#print(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 21999  | total loss: \u001b[1m\u001b[32m0.38774\u001b[0m\u001b[0m | time: 0.068s\n",
      "| Adam | epoch: 500 | loss: 0.38774 - acc: 0.7783 -- iter: 129/131\n",
      "Training Step: 22000  | total loss: \u001b[1m\u001b[32m0.41046\u001b[0m\u001b[0m | time: 0.070s\n",
      "| Adam | epoch: 500 | loss: 0.41046 - acc: 0.7671 -- iter: 131/131\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "# reset underlying graph data\n",
    "tf.reset_default_graph()\n",
    "# Build neural network\n",
    "#shape will be 2D array of 1*length of a single train list\n",
    "input_data = tflearn.input_data(shape=[None, len(train_x[0])])\n",
    "#create fully connected layer with 16 neurons\n",
    "#  6 is an arbitrary value chosen according to the number of bits you want to \n",
    "#be able to compress your network's trained parameters into.\n",
    "layer1 = tflearn.fully_connected(input_data,16,activation='ReLU')\n",
    "layer2 = tflearn.fully_connected(layer1,16,activation='relu6')\n",
    "net = tflearn.fully_connected(layer1, len(train_y[0]), activation='softmax')\n",
    "net = tflearn.regression(net)\n",
    "\n",
    "# Define model and setup tensorboard\n",
    "model = tflearn.DNN(net, tensorboard_dir='tflearn_logs')\n",
    "# Start training (apply gradient descent algorithm)\n",
    "model.fit(train_x, train_y, n_epoch=500, batch_size=3, show_metric=True,validation_batch_size=0.3)\n",
    "model.save('model.tflearn')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save all of our data structures\n",
    "import pickle\n",
    "pickle.dump( {'words':words, 'classes':classes, 'train_x':train_x, 'train_y':train_y}, open( \"training_data\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trained_data = pickle.load( open( \"training_data\", \"rb\" ) )\n",
    "words = trained_data['words']\n",
    "classes = trained_data['classes']\n",
    "train_x = trained_data['train_x']\n",
    "train_y = trained_data['train_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0902 15:07:11.276221  9388 deprecation.py:323] From c:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    }
   ],
   "source": [
    "# load our saved model\n",
    "model.load('./model.tflearn')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_sentence(sentence):\n",
    "    # tokenize the pattern\n",
    "    sentence_words = nltk.word_tokenize(sentence)\n",
    "    # stem each word\n",
    "    sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n",
    "    return sentence_words\n",
    "\n",
    "# return bag of words array: 0 or 1 for each word in the bag that exists in the sentence\n",
    "def make_vocab(sentence, words, show_details=False):\n",
    "    # tokenize the pattern\n",
    "    sentence_words = clean_up_sentence(sentence)\n",
    "    #vocabulary of words\n",
    "    vocab = [0]*len(words)  \n",
    "    for s in sentence_words:\n",
    "        for i,w in enumerate(words):\n",
    "            if w == s: \n",
    "                vocab[i] = 1\n",
    "                if show_details:\n",
    "                    print (\"found in bag: %s\" % w)\n",
    "\n",
    "    return(np.array(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "p = make_vocab(\"is your shop open today?\", words)\n",
    "print (p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a data structure to hold user context\n",
    "context = {}\n",
    "#minimum probability we accept that the bot is sure of what it says, in this case bot must be at least \n",
    "#10% sure before it replies\n",
    "ERROR_THRESHOLD = 0.60\n",
    "def classify(sentence):\n",
    "    # generate probabilities from the model\n",
    "    results = model.predict([make_vocab(sentence, words)])[0]\n",
    "    # filter out predictions below a threshold\n",
    "    results = [[k,v] for k,v in enumerate(results) if v>ERROR_THRESHOLD]\n",
    "    # sort by strength of probability/descending order\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    classify_result = []\n",
    "    for result in results:\n",
    "        classify_result.append((classes[result[0]], result[1]))\n",
    "    # return tuple of intent and probability\n",
    "    return classify_result\n",
    "\n",
    "def response(sentence, userID='user', show_details=False):\n",
    "    results = classify(sentence)\n",
    "    # if we have a classification then find the matching intent tag\n",
    "    if results:\n",
    "        # loop as long as there are matches to process\n",
    "        while results:\n",
    "            for intent in intents['intents']:\n",
    "                # find a tag matching the first result\n",
    "                if intent['tag'] == results[0][0]:\n",
    "                   #if the array json object contains a value of 'context_set', set that to the user session\n",
    "                    if 'context_set' in intent:\n",
    "                        if show_details: print ('context:', intent['context_set'])\n",
    "                        context[userID] = intent['context_set']\n",
    "\n",
    "                    # check if this intent is contextual and applies to this user's conversation\n",
    "                    if not 'context_filter' in intent or \\\n",
    "                        (userID in context and 'context_filter' in intent and intent['context_filter'] == context[userID]):\n",
    "                        if show_details: print ('tag:', intent['tag'])\n",
    "                        # a random response from the intent\n",
    "                        return str(random.choice(intent['responses']))\n",
    "\n",
    "            results.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context: \n",
      "tag: greeting\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hi there, how can I help?'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response(\" there!\", show_details=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "response('today')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('transaction', 0.9930328)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify('whiii doa youd consult d ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello too, we are Fintech, how may we help'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'does the log message displayed contain HARD or SOFT at the ending?'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response('i have some erors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('finfinancials', 0.99998665)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify(\"explain finfinancials?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user': 'error_finder'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0902 17:06:42.329814  9388 _internal.py:122]  * Running on http://10.100.0.177:8081/ (Press CTRL+C to quit)\n",
      "I0902 17:06:57.608074  7372 _internal.py:122] 10.100.0.177 - - [02/Sep/2019 17:06:57] \"\u001b[37mGET /process?message=ok HTTP/1.1\u001b[0m\" 200 -\n",
      "I0902 17:15:49.622996   640 _internal.py:122] 10.100.0.177 - - [02/Sep/2019 17:15:49] \"\u001b[37mGET /process?message=BYE%0A HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    }
   ],
   "source": [
    "# Flask is a micro framework for Python aiming to provide the flexibility of creating web sites and APIs.\n",
    "from flask import Flask, request,render_template\n",
    "from flask_cors import CORS\n",
    "\n",
    "app = Flask(__name__)\n",
    "app.static_folder = 'static'\n",
    "#use the cors module to allow unknown sources to access the process in the server's ip and get responses. This obvously needs\n",
    "#some alteration for security purposes\n",
    "CORS(app)\n",
    "#default route\n",
    "@app.route(\"/\") \n",
    "def home():\n",
    "    return render_template(\"home.html\") \n",
    "\n",
    "@app.route('/process') \n",
    "def get_bot_response():\n",
    "    userText = request.args['message'] \n",
    "    bot_response=response(userText) \n",
    "    if(bot_response is None): \n",
    "        bot_response=random.choice(['Sorry, I fail to understand that, Im a bot','Please rephrase the question, im just a bot']) \n",
    "    return bot_response \n",
    "\n",
    "#run flask session on this host\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='10.100.0.177',port=8081)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
