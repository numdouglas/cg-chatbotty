{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0829 11:53:12.843653  8180 deprecation_wrapper.py:119] From c:\\program files\\python37\\lib\\site-packages\\tflearn\\helpers\\summarizer.py:9: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
      "\n",
      "W0829 11:53:12.848667  8180 deprecation_wrapper.py:119] From c:\\program files\\python37\\lib\\site-packages\\tflearn\\helpers\\trainer.py:25: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "W0829 11:53:12.861199  8180 deprecation_wrapper.py:119] From c:\\program files\\python37\\lib\\site-packages\\tflearn\\collections.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "W0829 11:53:12.868218  8180 deprecation_wrapper.py:119] From c:\\program files\\python37\\lib\\site-packages\\tflearn\\config.py:123: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "W0829 11:53:12.880250  8180 deprecation_wrapper.py:119] From c:\\program files\\python37\\lib\\site-packages\\tflearn\\config.py:129: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
      "\n",
      "W0829 11:53:12.881253  8180 deprecation_wrapper.py:119] From c:\\program files\\python37\\lib\\site-packages\\tflearn\\config.py:131: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n"
     ]
    }
   ],
   "source": [
    "# things we need for NLP\n",
    "# NLTK is a leading platform for building Python programs to work with human language data.\n",
    "# It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet,\n",
    "# along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing,\n",
    "# and semantic reasoning.\n",
    "import nltk\n",
    "# A computer program or subroutine that stems word may be called a stemming program, stemming algorithm, or stemmer.\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "# things we need for Tensorflow\n",
    "# Numpy is a library for the Python programming language, adding \n",
    "# support for large, multi-dimensional arrays and matrices, along with a \n",
    "# large collection of high-level mathematical functions thttp://localhost:8888/notebooks/Documents/Notes/cg%20chatty%202/cg%20chatbotty/cg%20chatty.ipynb#o operate on these arrays.\n",
    "import numpy as np\n",
    "# TFlearn is a modular and transparent deep learning library built on top of Tensorflow.\n",
    "# It was designed to provide a higher-level API to TensorFlow in order to facilitate and speed-up experimentations,\n",
    "# while remaining fully transparent and compatible with it.\n",
    "import tflearn\n",
    "# Tensorflow is an open source machine learning library, which is used to develop and train ML models\n",
    "import tensorflow as tf\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import our chat-bot intents file\n",
    "import json\n",
    "intent_file= open('intents.json','r')\n",
    "intents = json.load(intent_file)\n",
    "intent_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131 documents\n",
      "28 classes ['EntSolutions', 'get_services', 'soft_issues_response_soft_antivirus', 'departments', 'soft_issues_response_noantivirus', 'bug_per', 'fins_trans', 'hours', 'soft_issues_response_errors', 'thanks', 'opentoday', 'ent_trans', 'finfinancials', 'management_structure', 'soft_issues', 'soft_issues_response_hard', 'payments', 'bug_resolve_affirmed', 'transaction', 'payments_n_clearing', 'location_question', 'get_info', 'services', 'Ebanking', 'greeting', 'e_trans', 'goodbye', 'clearing_trans']\n",
      "116 unique stemmed words ['posit', 'is', 'problem', 'yep', 'goodby', 'fintech', 'yo', 'no', 'deal', 'sort', 'on', 'a', 'clos', 'expery', 'hour', 'can', 'anyon', 'much', 'exceiv', 'phys', 'good', 'let', 'mor', 'the', 'er', 'tour', 'for', 'hav', 'that', 'tel', 'bug', 'man', 'tak', 'field', 'out', 'yeah', 'program', 'visit', 'loc', 'i', 'to', 'up', 'find', 'nee', 'hierachy', 'what', 'issu', 'depart', 'help', 'provid', 'am', 'ok', 'op', 'who', 'consult', 'ther', 'solv', 'clear', 'settl', 'wil', 'ebank', 'opt', 'hard', 'ceo', 'inform', 'neg', 'hello', 'you', 'ar', 'hi', ',', 'has', 'of', 'today', 'end', 'structure', 'explain', 'enlight', '2', 'cash', 'may', 'thank', 'how', 'day', 'pay', 'bye', 'in', 'want', 'serv', 'wher', 'mastercard', 'about', 'with', 'acceiv', 'an', 'softw', 'do', 'enterpr', 'keny', 'off', 'it', 'read', 'see', 'know', 'nop', 'lat', 'perform', 'ye', 'org', 'when', \"'s\", 'me', 'credit', 'card', 'near', 'finfin']\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "classes = []\n",
    "references = []\n",
    "words_to_ignore = ['?','!']\n",
    "# loop through each sentence in our intents patterns\n",
    "for intent in intents['intents']:\n",
    "    #note arr['name'] is visiting that array by name \n",
    "    for pattern in intent['question_patterns']:\n",
    "        # add to list containing the classes of words\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])\n",
    "        # tokenize each word in the sentence\n",
    "#Tokenization is the act of breaking up a sequence of strings into pieces such as words, keywords, phrases, symbols and other elements called tokens\n",
    "        word = nltk.word_tokenize(pattern)\n",
    "        # append word to our words list\n",
    "        words.extend(word)\n",
    "        # add to documents in our corpus \n",
    "#         A corpus is a collection of machine-readable texts that have been produced in a natural communicative setting\n",
    "#in the references multidim list of bot, append a list of words at index 0 and a list of contents of intents with key 'tag'\n",
    "        references.append((word, intent['tag']))\n",
    "        \n",
    "\n",
    "# stem and lower each word and remove duplicates\n",
    "words = [ stemmer.stem(word.lower()) for word in words if word not in words_to_ignore]\n",
    "#set method removes the duplicates in the words and classes obtained\n",
    "words = list(set(words))\n",
    "classes = list(set(classes))\n",
    "\n",
    "print (len(references), \"documents\")\n",
    "print (len(classes), \"classes\", classes)\n",
    "print (len(words), \"unique stemmed words\", words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our training data array bags\n",
    "training = []\n",
    "output = []\n",
    "# create an empty array for our output\n",
    "null_output = [0] * len(classes)\n",
    "#A vocabulary is a set of all words known to our bot\n",
    "# training set, vocabulary of words for each sentence\n",
    "#In the following method, we will convert the list of words we have to an array of tensors that tensorflow can use\n",
    "for ref in references:\n",
    "    # initialize our vocabulary of words\n",
    "    vocab = []\n",
    "    # list of tokenized words for the pattern\n",
    "    pattern_words = ref[0]\n",
    "    # stem each word, to its root(s) :-)\n",
    "    pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]\n",
    "    # create our vocabulary of words array\n",
    "    for w in words:\n",
    "        vocab.append(1) if w in pattern_words else vocab.append(0)\n",
    "\n",
    "    # output is a '0' for inactive and '1' for active tag\n",
    "    output_row = list(null_output)\n",
    "    output_row[classes.index(ref[1])] = 1\n",
    "#append to multidim array training the vocabulary of root forms and the row they each correspond to\n",
    "    training.append([vocab, output_row])\n",
    "\n",
    "# shuffle our features and turn into np.array\n",
    "#shuffling will reduce the variance and overfitting\n",
    "random.shuffle(training)\n",
    "training = np.array(training)\n",
    "\n",
    "# create train and test lists\n",
    "#the : stands for everything from the beginning to end , 0 represents the dimension, the 1st in 1d array\n",
    "train_x = list(training[:,0])\n",
    "train_y = list(training[:,1])\n",
    "#print(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 21999  | total loss: \u001b[1m\u001b[32m0.34411\u001b[0m\u001b[0m | time: 0.069s\n",
      "| Adam | epoch: 500 | loss: 0.34411 - acc: 0.8318 -- iter: 129/131\n",
      "Training Step: 22000  | total loss: \u001b[1m\u001b[32m0.30971\u001b[0m\u001b[0m | time: 0.071s\n",
      "| Adam | epoch: 500 | loss: 0.30971 - acc: 0.8487 -- iter: 131/131\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "# reset underlying graph data\n",
    "tf.reset_default_graph()\n",
    "# Build neural network\n",
    "#shape will be 2D array of 1*length of a single train list\n",
    "input_data = tflearn.input_data(shape=[None, len(train_x[0])])\n",
    "#create fully connected layer with 16 neurons\n",
    "#  6 is an arbitrary value chosen according to the number of bits you want to \n",
    "#be able to compress your network's trained parameters into.\n",
    "layer1 = tflearn.fully_connected(input_data,16,activation='ReLU')\n",
    "layer2 = tflearn.fully_connected(layer1,16,activation='relu6')\n",
    "net = tflearn.fully_connected(layer1, len(train_y[0]), activation='softmax')\n",
    "net = tflearn.regression(net)\n",
    "\n",
    "# Define model and setup tensorboard\n",
    "model = tflearn.DNN(net, tensorboard_dir='tflearn_logs')\n",
    "# Start training (apply gradient descent algorithm)\n",
    "model.fit(train_x, train_y, n_epoch=500, batch_size=3, show_metric=True,validation_batch_size=0.3)\n",
    "model.save('model.tflearn')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save all of our data structures\n",
    "import pickle\n",
    "pickle.dump( {'words':words, 'classes':classes, 'train_x':train_x, 'train_y':train_y}, open( \"training_data\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trained_data = pickle.load( open( \"training_data\", \"rb\" ) )\n",
    "words = trained_data['words']\n",
    "classes = trained_data['classes']\n",
    "train_x = trained_data['train_x']\n",
    "train_y = trained_data['train_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0829 11:53:14.019701  8180 deprecation.py:323] From c:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    }
   ],
   "source": [
    "# load our saved model\n",
    "model.load('./model.tflearn')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_sentence(sentence):\n",
    "    # tokenize the pattern\n",
    "    sentence_words = nltk.word_tokenize(sentence)\n",
    "    # stem each word\n",
    "    sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n",
    "    return sentence_words\n",
    "\n",
    "# return bag of words array: 0 or 1 for each word in the bag that exists in the sentence\n",
    "def make_vocab(sentence, words, show_details=False):\n",
    "    # tokenize the pattern\n",
    "    sentence_words = clean_up_sentence(sentence)\n",
    "    #vocabulary of words\n",
    "    vocab = [0]*len(words)  \n",
    "    for s in sentence_words:\n",
    "        for i,w in enumerate(words):\n",
    "            if w == s: \n",
    "                vocab[i] = 1\n",
    "                if show_details:\n",
    "                    print (\"found in bag: %s\" % w)\n",
    "\n",
    "    return(np.array(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "p = make_vocab(\"is your shop open today?\", words)\n",
    "print (p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a data structure to hold user context\n",
    "context = {}\n",
    "#minimum probability we accept that the bot is sure of what it says, in this case bot must be at least \n",
    "#10% sure before it replies\n",
    "ERROR_THRESHOLD = 0.60\n",
    "def classify(sentence):\n",
    "    # generate probabilities from the model\n",
    "    results = model.predict([make_vocab(sentence, words)])[0]\n",
    "    # filter out predictions below a threshold\n",
    "    results = [[k,v] for k,v in enumerate(results) if v>ERROR_THRESHOLD]\n",
    "    # sort by strength of probability/descending order\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    classify_result = []\n",
    "    for result in results:\n",
    "        classify_result.append((classes[result[0]], result[1]))\n",
    "    # return tuple of intent and probability\n",
    "    return classify_result\n",
    "\n",
    "def response(sentence, userID='user', show_details=False):\n",
    "    results = classify(sentence)\n",
    "    # if we have a classification then find the matching intent tag\n",
    "    if results:\n",
    "        # loop as long as there are matches to process\n",
    "        while results:\n",
    "            for intent in intents['intents']:\n",
    "                # find a tag matching the first result\n",
    "                if intent['tag'] == results[0][0]:\n",
    "                   #if the array json object contains a value of 'context_set', set that to the user session\n",
    "                    if 'context_set' in intent:\n",
    "                        if show_details: print ('context:', intent['context_set'])\n",
    "                        context[userID] = intent['context_set']\n",
    "\n",
    "                    # check if this intent is contextual and applies to this user's conversation\n",
    "                    if not 'context_filter' in intent or \\\n",
    "                        (userID in context and 'context_filter' in intent and intent['context_filter'] == context[userID]):\n",
    "                        if show_details: print ('tag:', intent['tag'])\n",
    "                        # a random response from the intent\n",
    "                        return str(random.choice(intent['responses']))\n",
    "\n",
    "            results.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "response(\" there!\", show_details=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "response('today')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify('whiii doa youd consult d ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "response('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'please read the log message displayed on the package console: is it ending with HARD or SOFT?'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response('i have some erors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('soft_issues_response_hard', 0.9999534)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify(\"explain finfinancials?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user': 'error_finder'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0829 15:44:51.420146  8180 _internal.py:122]  * Running on http://10.100.0.177:8081/ (Press CTRL+C to quit)\n",
      "I0829 15:44:52.899079  1524 _internal.py:122] 10.100.0.177 - - [29/Aug/2019 15:44:52] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
      "I0829 15:44:52.932669 16180 _internal.py:122] 10.100.0.177 - - [29/Aug/2019 15:44:52] \"\u001b[33mGET /static/static/speech-balloon.svg HTTP/1.1\u001b[0m\" 404 -\n"
     ]
    }
   ],
   "source": [
    "# Flask is a micro framework for Python aiming to provide the flexibility of creating web sites and APIs.\n",
    "from flask import Flask, request,render_template\n",
    "from flask_cors import CORS\n",
    "\n",
    "app = Flask(__name__)\n",
    "app.static_folder = 'static'\n",
    "#use the cors module to allow unknown sources to access the process in the server's ip and get responses. This obvously needs\n",
    "#some alteration for security purposes\n",
    "CORS(app)\n",
    "#default route\n",
    "@app.route(\"/\") \n",
    "def home():\n",
    "    return render_template(\"home.html\") \n",
    "\n",
    "@app.route('/process') \n",
    "def get_bot_response():\n",
    "    userText = request.args['message'] \n",
    "    bot_response=response(userText) \n",
    "    if(bot_response is None): \n",
    "        bot_response=random.choice(['Sorry, I fail to understand that, Im a bot','Please rephrase the question, im just a bot']) \n",
    "    return bot_response \n",
    "\n",
    "#run flask session on this host\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='10.100.0.177',port=8081)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0829 12:10:02.565681  8180 _internal.py:122]  * Running on http://localhost:9000/ (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "# Flask is a micro framework for Python aiming to provide the flexibility of creating web sites and APIs.\n",
    "from flask import Flask, request,render_template\n",
    "app = Flask(__name__)\n",
    "app.static_folder = 'static'\n",
    "@app.route('/')\n",
    "def index():\n",
    "\treturn render_template('index.html')\n",
    "@app.route('/process',methods=['post'])\n",
    "def process():\n",
    "\tuser_input=request.form['user_input']\n",
    "\tbot_response=response(user_input)\n",
    "\tif(bot_response is None):\n",
    "\t\tbot_response=random.choice(['Sorry, I fail to understand that, Im a bot','Please rephrase the question, im just a bot'])\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\treturn render_template('index.html',user_input=user_input,bot_response=bot_response)\n",
    "# if __name__ == '__main__':\n",
    "# \tapp.run()\n",
    "# #make the session last longer as jupyter does not provide the ability\n",
    "from werkzeug.serving import run_simple\n",
    "run_simple('localhost', 9000, app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "This event loop is already running",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-cef10a52b9e2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;31m#       await render(websocket, path)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0masyncio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_event_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_until_complete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwebsockets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mserve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrouter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'0.0.0.0'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m8765\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[0masyncio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_event_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_forever\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\asyncio\\base_events.py\u001b[0m in \u001b[0;36mrun_until_complete\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m    553\u001b[0m         \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_done_callback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_run_until_complete_cb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_forever\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    556\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mnew_task\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcancelled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\asyncio\\base_events.py\u001b[0m in \u001b[0;36mrun_forever\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    508\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    509\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 510\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'This event loop is already running'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    511\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mevents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_running_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    512\u001b[0m             raise RuntimeError(\n",
      "\u001b[1;31mRuntimeError\u001b[0m: This event loop is already running"
     ]
    }
   ],
   "source": [
    "from django.shortcuts import render, render_to_response\n",
    "from django.http import HttpResponse\n",
    "from django.views.decorators.csrf import csrf_exempt\n",
    "\n",
    "import asyncio\n",
    "import websockets\n",
    "# Train based on the english corpus\n",
    "\n",
    "#Already trained and it's supposed to be persistent\n",
    "#chatbot.train(\"chatterbot.corpus.english\")\n",
    "\n",
    "async def chat_receiver(websocket, path):\n",
    "    async for message in websocket:\n",
    "        message = json.loads(message)\n",
    "        text = message['text']\n",
    "\n",
    "        chat_response = chatbot.get_response(text).text\n",
    "        print(chat_response)\n",
    "        await websocket.send(json.dumps({'response': chat_response}))\n",
    "\n",
    "async def router(websocket, path):\n",
    "\tif path == \"/\":\n",
    "\t\tawait chat_receiver(websocket, path)\n",
    "\n",
    "    #the code below is how you add other path's\n",
    "    \n",
    "\t# elif path == \"/shade-area\":\n",
    "\t# \tawait get_shaded_area(websocket, path)\n",
    "\t# elif path == '/render':\n",
    "\t# \tawait render(websocket, path)\n",
    "\n",
    "asyncio.get_event_loop().run_until_complete(websockets.serve(router, '0.0.0.0', 8765))\n",
    "\n",
    "asyncio.get_event_loop().run_forever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from django.shortcuts import render, render_to_response\n",
    "from django.http import HttpResponse\n",
    "import json\n",
    "from django.views.decorators.csrf import csrf_exempt\n",
    "from django.conf import settings\n",
    "settings.configure()\n",
    "\n",
    "\n",
    "# Train based on the english corpus\n",
    "\n",
    "#Already trained and it's supposed to be persistent\n",
    "#chatbot.train(\"chatterbot.corpus.english\")\n",
    "\n",
    "@csrf_exempt\n",
    "def get_response(request):\n",
    "\tresponse = {'status': None}\n",
    "\n",
    "\tif request.method == 'POST':\n",
    "\t\tdata = json.loads(request.body)\n",
    "\t\tmessage = data['message']\n",
    "\n",
    "\t\tchat_response = \"that's fine\"\n",
    "\t\tresponse['message'] = {'text': chat_response, 'user': False, 'chat_bot': True}\n",
    "\t\tresponse['status'] = 'ok'\n",
    "\n",
    "\telse:\n",
    "\t\tresponse['error'] = 'no post data found'\n",
    "\n",
    "\treturn HttpResponse(\n",
    "\t\tjson.dumps(response),\n",
    "\t\t\tcontent_type=\"application/json\"\n",
    "\t\t)\n",
    "\n",
    "\n",
    "def home(request, template_name=\"home.html\"):\n",
    "\tcontext = {'title': 'Chatbot Version 1.0'}\n",
    "\treturn render_to_response(template_name, context)\n",
    "home('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
